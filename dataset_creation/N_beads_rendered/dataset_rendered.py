"""
dataset_rendered.py renders data generated by N_beads/dataset_test.py.
Code has very low GPU utilization (few percent), 
but runs reasonably fast so did not try to optimize. 

All code in JAX. 

Copyright Catherine Ji, 2025. 
"""

import jax.numpy as jnp
from jax import jit, vmap
import numpy as np
import os
import sys
import h5py
import logging
import argparse
import yaml 

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from utils import generate_dataset_render

################################################################################
# Parameter handling
################################################################################

def get_parameters_by_index(config, index):
    keys = [key for key in config.keys() if "directory" not in key]
    sizes = [len(config[key]) for key in keys]
    total_combinations = np.prod(sizes)
    if index >= total_combinations:
        raise IndexError("Index out of bounds for the given configuration.")
    params = {}

    # Divmod to get parameter values.
    for key in keys:
        size = len(config[key])
        index, value_index = divmod(index, size)
        params[key] = config[key][value_index]
    return params

################################################################################
# Rendering functions
################################################################################

@jit
def plot_beads_jax_slice(beads, X_MIN, X_MAX, WIDTH, i, bandwidth_factor):

    X_DELTA = X_MAX - X_MIN
    effective_bandwidth = (bandwidth_factor * 2 * X_DELTA / WIDTH)
    
    # Later summed along the num_beads dimension so only one channel.
    dist = (i[None, None, :] - beads[:, :, None])**2  # (batch_size, num_beads, width)
    probs = jnp.exp(-dist / (2 * effective_bandwidth**2))  # (batch_size, num_beads, width)

    # Prep for uint8 conversion.
    probs = (probs * 255) 
    return probs

@jit
def process_batch_vmap_slice(data_batch_x, data_batch_y,  X_MIN, X_MAX, WIDTH, i, bandwidth_factor):
    beads_x = data_batch_x  # (batch_size, num_beads, dim)
    beads_y = data_batch_y  # (batch_size, num_beads, dim)
    
    # Generate images for x_t and x_{t+1} beads.
    imgs_x = plot_beads_jax_slice(beads_x, X_MIN, X_MAX, WIDTH, i, bandwidth_factor)
    imgs_y = plot_beads_jax_slice(beads_y, X_MIN, X_MAX, WIDTH, i, bandwidth_factor)
    
    return imgs_x, imgs_y

################################################################################
# Main processing function
################################################################################

def process_and_save_gpu(data, file_path, dataset_name, MIN, MAX, WIDTH, i, bandwidth_factor, chunk_size=1024):
    """
    Main function for processing and saving data.
    """
    num_samples = data['x'].shape[0]

    # Initialize files.
    estimated_file_size_GB = num_samples * 2 * WIDTH / (1024**3)
    logging.info(f"Sum and slice")
    with h5py.File(file_path, 'a') as h5f:
        if dataset_name not in h5f:
            h5f.create_dataset(
                dataset_name,
                shape=(num_samples, 2, WIDTH, 1), # (timesteps, prev/future, WIDTH, HEIGHT=1) 
                dtype='uint8', # to have dataset fit in VRAM 
                chunks=(chunk_size, 2, WIDTH, 1), # (chunk, prev/future, WIDTH, HEIGHT=1)
            )
    logging.info(f"Filesize (GB): {estimated_file_size_GB}")

    # Set vmap function.
    batch_vmap = process_batch_vmap_slice
    batch_size = chunk_size  

    for start_idx in range(0, num_samples, batch_size):
        end_idx = min(start_idx + batch_size, num_samples)
        data_batch_x = data['x'][start_idx:end_idx].squeeze(-1)  # (batch_size, N, D)
        data_batch_y = data['y'][start_idx:end_idx].squeeze(-1)  # (batch_size, N, D)

        # Render beads w/ vmap.
        imgs_x, imgs_y = batch_vmap(data_batch_x, data_batch_y, MIN, MAX, WIDTH, i, bandwidth_factor)

        # Sum over beads so only one channel.
        imgs_x = np.sum(imgs_x, axis=1, keepdims=False) # (batch_size, width)
        imgs_y = np.sum(imgs_y, axis=1, keepdims=False) # (batch_size, width)

        # Clip to 0-255 for conversion to uint8.
        imgs_x = np.clip(imgs_x, 0, 255)
        imgs_y = np.clip(imgs_y, 0, 255)
        imgs_x = imgs_x.astype(np.uint8) # Rounds to nearest int 0-255 for uint8.
        imgs_y = imgs_y.astype(np.uint8) 

        with h5py.File(file_path, 'a') as h5f:
            h5f[dataset_name][start_idx:end_idx, 0, :, 0] = imgs_x # keep HEIGHT dimension so data is saved as WxH images.
            h5f[dataset_name][start_idx:end_idx, 1, :, 0] = imgs_y

################################################################################
# Main execution
################################################################################

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Parse config and job index.")
    parser.add_argument("--config_file", type=str, required=True, help="YAML config file (required)")
    parser.add_argument("--job_index", type=int, required=True, help="Index for parameter sweep")
    parser.add_argument("--slurm_id", type=str, required=True, help="SLURM array ID")
    parser.add_argument("--task_id", type=str, required=True, help="Task ID")

    args = parser.parse_args()

    # Load YAML config file.
    with open(args.config_file, 'r') as f:
        config = yaml.safe_load(f)

    # Get params based on job index.
    params = get_parameters_by_index(config, args.job_index)

    # Create save directory.
    directory = config["directory"]
    directory = os.path.join(directory, f"{args.slurm_id}")
    os.makedirs(directory, exist_ok=True)

    # Save config file to directory.
    with open(args.config_file, "r") as src:
        with open(directory + '/' + "config.yml", "w") as dst:
            dst.write(src.read())

    # Initialize parameters.
    num_frames_percentage = params["num_frames_percentage"] # Percentage of frames to render
    shift_displacements = params["shift_displacements"] # Whether to shift displacements for rendering so beads don't overlap.
    resolution = params["resolution"] # Resolution of the image
    bandwidth_factor = params["bandwidth_factor"] # Bandwidth factor for the Gaussian kernel
    filename_to_render = params["filename"] # List of filenames to render
    directory_to_render = config["directory_to_render"] # Directory to render
    filepath_to_render = os.path.join(directory_to_render, filename_to_render)

    # Create filename.
    filename = os.path.join(
        directory,
        f"rendered_resolution={resolution}_filename={filename_to_render}.h5"
    )

    # Get dataset from fully-observed dataset (see N_beads/dataset_test.py)
    # with shift displacements.
    train_dataset, val_dataset, epr = generate_dataset_render(filepath_to_render, num_samples_percentage=num_frames_percentage, shift_displacements=shift_displacements)

    # Precompute visualization constants and grids.
    min_train = jnp.min(train_dataset['x']) 
    min_test = jnp.min(val_dataset['x'])
    MIN = min(min_train, min_test)
    max_train = jnp.max(train_dataset['x'])
    max_test = jnp.max(val_dataset['x'])
    MAX = max(max_train, max_test)
    HEIGHT, WIDTH = resolution
    aspect_ratio = WIDTH / HEIGHT
    i = jnp.linspace(MIN, MAX, WIDTH)

    # Render test data. 
    process_and_save_gpu(val_dataset, filename, 'data_test', MIN, MAX, WIDTH, i, bandwidth_factor)
    logging.info("Finished rendering test data. Processing train data...")

    # Render train data. 
    process_and_save_gpu(train_dataset, filename, 'data_train', MIN, MAX, WIDTH, i, bandwidth_factor)
    logging.info("Finished rendering all data.")

    # Add EPR to dataset.
    with h5py.File(filename, 'a') as h5f:
        if 'EPR' not in h5f:
            h5f.create_dataset(
                'EPR',
                shape=(1),
                dtype='float32'
            )
        h5f['EPR'][:] = epr
    logging.info("Added EPR.")
